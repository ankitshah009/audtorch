

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introduction &mdash; audtorch Documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.png"/>
  
  
  

  

  
  
    

  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/audeering.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="audtorch.collate" href="../api-collate.html" />
    <link rel="prev" title="Changelog" href="../changelog.html" />
    
  

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          <a href="../index.html">
          
            <img src="../_static/images/audeering.png" class="logo" alt="audEERING"/>
          
          
            <span> audtorch</span>
          
          </a>

          
            
            
              <div class="version">
                v0.6.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../develop.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Preliminaries">Preliminaries</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Data-loading">Data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Feature-extraction">Feature extraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Data-augmentation">Data augmentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Stacking-data-augmentation-and-feature-extraction">Stacking data augmentation and feature extraction</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api-collate.html">audtorch.collate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-datasets.html">audtorch.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-metrics.html">audtorch.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-metrics-functional.html">audtorch.metrics.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-samplers.html">audtorch.samplers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-transforms.html">audtorch.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-transforms-functional.html">audtorch.transforms.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-utils.html">audtorch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../refs.html">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Index</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">audtorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Introduction</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/audeering/audtorch/blob/v0.6.3/docs/tutorials/introduction.ipynb" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="Introduction">
<h1>Introduction<a class="headerlink" href="#Introduction" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial, we will see how one can use <code class="docutils literal notranslate"><span class="pre">audtorch</span></code> to rapidly speed up the development of audio-based deep learning applications.</p>
<div class="section" id="Preliminaries">
<h2>Preliminaries<a class="headerlink" href="#Preliminaries" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/">PyTorch</a> already has an inteface for data sets, aptly called <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset">Dataset</a></p></li>
<li><p>It then wraps this interface with a <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">DataLoader</a> that efficiently allows us to loop through the data in parallel, and takes care of the random order as well</p></li>
<li><p>All we need to do is implement the <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset">Dataset</a> interface to get the input for the model and the labels</p></li>
<li><p><strong>However</strong>, it is not easy for beginners to see how one can go from a bunch of files in their hard drive, to the features that will be used as input in a machine learning model</p></li>
<li><p><strong>Thankfully</strong>, <code class="docutils literal notranslate"><span class="pre">audtorch</span></code> is there to take of all that for you :-)</p></li>
</ul>
<p>Before you start, you might want to familiarize yourselves with <a class="reference external" href="https://pytorch.org/docs/stable/data.html">PyTorch’s data pipeline</a></p>
</div>
<div class="section" id="Data-loading">
<h2>Data loading<a class="headerlink" href="#Data-loading" title="Permalink to this headline">¶</a></h2>
<p>We are going to start with loading the necessary data.</p>
<p><code class="docutils literal notranslate"><span class="pre">audtorch</span></code> offers a growing <a class="reference external" href="https://audeering.github.io/audtorch/api-datasets.html">collection of data sets</a>. Normally, using this interface requires one to have that particular data set on their hard drive. Some of them even support downloading from their original source.</p>
<p>We will be using the Berlin Database of Emotional Speech (EmoDB) for this tutorial. For convenience, we have included two of its files in a sub-directory. We recommend you to get the whole data base from its <a class="reference external" href="http://www.emodb.bilderbar.info/navi.html">original website</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">audtorch</span>
<span class="kn">import</span> <span class="nn">IPython.display</span> <span class="k">as</span> <span class="nn">ipd</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">audtorch</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">EmoDB</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s1">&#39;data/emodb&#39;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">ipd</span><span class="o">.</span><span class="n">Audio</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>That’s it really. Up to this point, <code class="docutils literal notranslate"><span class="pre">audtorch</span></code> does not add much to the PyTorch’s data API, which is already quite advanced anyway.</p>
</div>
<div class="section" id="Feature-extraction">
<h2>Feature extraction<a class="headerlink" href="#Feature-extraction" title="Permalink to this headline">¶</a></h2>
<p>Feature extraction is the first important benefit of using <code class="docutils literal notranslate"><span class="pre">audtorch</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">audtorch</span></code> collects an ever growing set of <a class="reference external" href="https://audeering.github.io/audtorch/api-transforms.html#">feature transformation and data pre-processing utilities</a>. That way you don’t need to worry too much about getting your data pipeline ready, but you can quickly start with the cool modelling part.</p>
<p>A typical kind of features used in the audio domain, are spectral features. Audio signals are analyzed with respect to their frequency content using something called a <a class="reference external" href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier transform</a>.</p>
<p>Moreover, since that content changes over time, we normally use a <a class="reference external" href="https://en.wikipedia.org/wiki/Short-time_Fourier_transform">short-time Fourier Transform</a>. This leads then to the generation of a so-called <a class="reference external" href="https://en.wikipedia.org/wiki/Spectrogram">spectrogram</a>, which is nothing more than an image representation of the frequency content of a signal over time.</p>
<p>We assume that the reader is already familiar with this terminology. What’s important to point out, is that <code class="docutils literal notranslate"><span class="pre">audtorch</span></code> is designed to allow for easy usage of those features in a typical <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> workflow. Below, we see an example of how a feature extraction transform is defined:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">spec</span> <span class="o">=</span> <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Spectrogram</span><span class="p">(</span>
    <span class="n">window_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.025</span> <span class="o">*</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">),</span>
    <span class="n">hop_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.010</span> <span class="o">*</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">spec</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>By plotting the spectrogram, we see what frequency content our signal has over time.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">spectrogram</span> <span class="o">=</span> <span class="n">spec</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">spectrogram</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">invert_yaxis</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>The above image looks mostly empty. That’s why we have a lot of content with very low power that is dominated by the presence of a few frequencies where most of the signal’s power is concentrated.</p>
<p>It is typical to compute the logarithm of the spectrogram to reveal more information. That squashes the input and reveals previously hidden structure in other frequency bands. Incidentally, this squashing reduces the dynamic range of the resulting image, which makes our input more suitable for deep neural network training.</p>
<p><code class="docutils literal notranslate"><span class="pre">audtorch</span></code> provides a nice wrapper function for <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html">numpy’s log</a> to simplify things.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">lg</span> <span class="o">=</span> <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Log</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lg</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">log_spectrogram</span> <span class="o">=</span> <span class="n">lg</span><span class="p">(</span><span class="n">spectrogram</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">log_spectrogram</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">invert_yaxis</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>This image shows that there is a lot more going on in our signal than we previously thought.</p>
<p>In general, we recommend to always start with a preliminary data analysis before you jump into modelling to ensure you have the proper understanding of your problem.</p>
<p><code class="docutils literal notranslate"><span class="pre">audtorch</span></code> is here to help you with that, and another useful feature is that it allows you to stack multiple transforms in a <a class="reference external" href="https://audeering.github.io/audtorch/api-transforms.html#audtorch.transforms.Compose">Compose transform</a>. Below, we stack together the spectrogram and the log transforms to form a single object.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">t</span> <span class="o">=</span> <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Spectrogram</span><span class="p">(</span>
            <span class="n">window_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.025</span> <span class="o">*</span> <span class="mi">16000</span><span class="p">),</span>
            <span class="n">hop_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.010</span> <span class="o">*</span> <span class="mi">16000</span><span class="p">)</span>
        <span class="p">),</span>
        <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Log</span><span class="p">()</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">invert_yaxis</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>This stacking can continue <em>ad infinum</em>, as seen below with the <a class="reference external" href="https://audeering.github.io/audtorch/api-transforms.html#standardize">Standardize transform</a>.</p>
<p>Make sure to always stay up to date with <a class="reference external" href="https://audeering.github.io/audtorch/api-transforms.html">all the transforms offered by audtorch</a>!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">t</span> <span class="o">=</span> <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Spectrogram</span><span class="p">(</span>
            <span class="n">window_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.025</span> <span class="o">*</span> <span class="mi">16000</span><span class="p">),</span>
            <span class="n">hop_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.010</span> <span class="o">*</span> <span class="mi">16000</span><span class="p">)</span>
        <span class="p">),</span>
        <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Log</span><span class="p">(),</span>
        <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Standardize</span><span class="p">()</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">invert_yaxis</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Data-augmentation">
<h2>Data augmentation<a class="headerlink" href="#Data-augmentation" title="Permalink to this headline">¶</a></h2>
<p>One of the most crucial aspects of recent deep learning successes is arguably data augmentation. Roughly, this means increasing the sampling of your input space by creating slightly different copies of the original input without changing the label.</p>
<p>In the image domain, people use a variety of transforms, such as:</p>
<ul class="simple">
<li><p>Adding noise</p></li>
<li><p>Cropping</p></li>
<li><p>Rotating</p></li>
<li><p>Etc.</p></li>
</ul>
<p>Things are not so easy in the audio domain. Rotation, for example, does not make any sense for spectrogram features, since the two axes are not interchangeable. In general, the community seems to use the following transforms:</p>
<ul class="simple">
<li><p>Noise</p></li>
<li><p>Time/frequency masking</p></li>
<li><p>Pitch shifting</p></li>
<li><p>Etc.</p></li>
</ul>
<p>An important feature of <code class="docutils literal notranslate"><span class="pre">audtorch</span></code> is making these transformations very easy to use in practice. In the following example, we will be using <a class="reference external" href="https://audeering.github.io/audtorch/api-transforms.html#randomadditivemix">RandomAdditiveMix</a>. This transforms allows you to randomly mix audio samples with a noise data set of your choice (e.g. a large audio data set like <a class="reference external" href="https://audeering.github.io/audtorch/api-datasets.html#audioset">AudioSet</a>).</p>
<p>In this example, we will use a built-in data set, <a class="reference external" href="https://audeering.github.io/audtorch/api-datasets.html#whitenoise">WhiteNoise</a>, which simply creates a random white noise signal every time it is called.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">random_mix</span> <span class="o">=</span> <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">RandomAdditiveMix</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">audtorch</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">WhiteNoise</span><span class="p">(</span><span class="n">sampling_rate</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random_mix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>You can see that this transforms modifies the audio signal itself, by adding this “static” TV noise to our original signal. Obviously though, the emotion of the speaker remains the same. This is a very practical way to augment your training set without changing the labels.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">IPython.display</span> <span class="k">as</span> <span class="nn">ipd</span>
<span class="n">ipd</span><span class="o">.</span><span class="n">Audio</span><span class="p">(</span><span class="n">random_mix</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">rate</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="Stacking-data-augmentation-and-feature-extraction">
<h3>Stacking data augmentation and feature extraction<a class="headerlink" href="#Stacking-data-augmentation-and-feature-extraction" title="Permalink to this headline">¶</a></h3>
<p>What is really important, is that <code class="docutils literal notranslate"><span class="pre">audtorch</span></code> allows us to do simultaneous data augmentation and feature extraction <strong>on-the-fly</strong>.</p>
<p>This is very useful in the typical case where we run the same training samples multiple times through the network (i.e. when we train for multiple epochs), and would like to slightly change the input every time. All we have to do is stack our data augmentation transforms on top of our feature extraction ones.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">t</span> <span class="o">=</span> <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">RandomAdditiveMix</span><span class="p">(</span>
            <span class="n">dataset</span><span class="o">=</span><span class="n">audtorch</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">WhiteNoise</span><span class="p">(</span><span class="n">sampling_rate</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">),</span>
            <span class="n">expand_method</span><span class="o">=</span><span class="s1">&#39;multiple&#39;</span>
        <span class="p">),</span>
        <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Spectrogram</span><span class="p">(</span>
            <span class="n">window_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.025</span> <span class="o">*</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">),</span>
            <span class="n">hop_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.010</span> <span class="o">*</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">)</span>
        <span class="p">),</span>
        <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Log</span><span class="p">(),</span>
        <span class="n">audtorch</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Standardize</span><span class="p">()</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can clearly see how this spectrogram seems noisier than the one we had before. Hopefully, this will be enough to make our classifier generalize better!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">invert_yaxis</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../api-collate.html" class="btn btn-neutral float-right" title="audtorch.collate" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../changelog.html" class="btn btn-neutral float-left" title="Changelog" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div>
    <p>
        
        
        
          Built with <a href="https://www.sphinx-doc.org/en/master/">Sphinx</a> on 2020/10/30 using the <a href="https://github.com/audeering/sphinx-audeering-theme/">audEERING theme</a>
        
    </p>
  </div>

  <div role="contentinfo">
    <p>
        
      &copy; 2019 audEERING GmbH
    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"></script>
        <script src="../_static/katex_autorenderer.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>